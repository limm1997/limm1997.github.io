---
layout:     post                    # 使用的布局
title:      Deep Learning               # 标题 
subtitle:   #副标题
date:       2019-01-04              # 时间
author:     Doublefierce                      # 作者
header-img: img/bg-post.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---

### 神经网络

>  https://blog.csdn.net/walilk/article/details/50278697

### 损失函数

>**一文读懂机器学习常用损失函数（Loss Function）**
>https://www.cnblogs.com/guoyaohua/p/9217206.html
>**学点基本功：机器学习常用损失函数小结**
>https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/101875924
>**机器学习中的损失函数 （着重比较：hinge loss vs softmax loss）**
>https://blog.csdn.net/u010976453/article/details/78488279
>**目标函数，损失函数和代价函数**
>https://blog.csdn.net/qq_28448117/article/details/79199835

### 交叉熵与Softmax函数
> **交叉熵（Cross-Entropy）**
> https://blog.csdn.net/rtygbwwwerr/article/details/50778098
> **一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉**
> https://blog.csdn.net/tsyccnh/article/details/79163834
> **交叉熵代价函数(损失函数)及其求导推导**
> https://blog.csdn.net/jasonzzj/article/details/52017438
> **详解softmax函数以及相关求导过程**
> https://blog.csdn.net/pql925/article/details/81010836
> **简单易懂的softmax交叉熵损失函数求导**
> https://blog.csdn.net/qian99/article/details/78046329

### 激活函数
> 激活函数实现的是一对一的变换，即用相同的函数对输入向量的每个分量进行映射，得到输出向量，输入和输出向量的维数相同。
> **理解神经网络的激活函数**
> https://mp.weixin.qq.com/s/ix5RJcGQ7SMGeU5Yz4z2dg
> **常用激活函数（激励函数）理解与总结**
> https://www.jianshu.com/writer#/notebooks/38075299/notes/57457007/preview

### 优化方法
> **深入理解优化器Optimizer算法（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）**
> https://www.cnblogs.com/guoyaohua/p/8780548.html
> **优化方法总结：SGD，Momentum，AdaGrad，RMSProp，Adam**
> https://blog.csdn.net/u010089444/article/details/76725843

### Dropout
>开篇明义，dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。
>https://blog.csdn.net/stdcoutzyx/article/details/49022443

### Batch_size
> **深度学习中的batch size 以及learning rate参数理解**
> https://blog.csdn.net/lemonaha/article/details/72773056

### Fine-tuning
> **什么是fine-tuning？**
> https://blog.csdn.net/weixin_42137700/article/details/82107208

### 调参
> **深度学习网络调参技巧**
> https://zhuanlan.zhihu.com/p/24720954?utm_source=wechat_session&utm_medium=social&utm_oi=637252858509135872