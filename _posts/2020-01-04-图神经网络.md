---
layout:     post                    # 使用的布局
title:      Graph Neural Network               # 标题 
subtitle:   #副标题
date:       2019-01-04              # 时间
author:     Doublefierce                      # 作者
header-img: img/bg-post.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - GNN
    - GCN
---

### GNN【1，2，3，4】

1. 什么是embedding，为什么要做embedding？

   Embedding在数学上是一个函数，f: X->Y, 即将一个空间的点映射到另一个空间，通常是从高维抽象的空间映射到低维具象的空间；

   一般映射到低维空间的表示具有分布式稠密表示的特性；

   抽象的事物应该有一个低维的表示，比如我们看到一张含有小狗的照片，它底层的表示就是像素值。同样我们看到dog这个单词时，它也应该有更低维的表示

   计算机和神经网络善于处理低纬度信息

   解决One-hot编码问题：One-hot编码是一种特殊的高维到低维的映射，具有稀疏性，且向量长度较长并随着样本集变化而变化。one-hot 编码是一种讨巧的编码方式，一般无法表示两个实体之间的相关性，embedding可以一定程度上解决这些问题。

###  GCN 【5，6，7，8】

1. 什么是离散卷积？CNN中卷积发挥什么作用？

   离散卷积本质就是一种加权求和

   通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取

   卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入可以优化的卷积参数

2. GCN中的Graph指的是什么？为什么要研究GCN?

   CNN处理的图像或者视频数据中像素点是排列成很整齐的矩阵

   Graph是指数学（图论）中用定点和边建立相应关系的拓扑图

   研究GCN的原因：

   （1）传统的离散卷积在Non Euclidean Structure 的数据上无法保持平移不变性

   （2）希望在这样的数据结构（拓扑图）上有效的提取空间特征来进行机器学习

   （3）广义上来讲 任何数据在赋范空间内都可以建立拓扑关联，谱聚类就是应用了这样的思想，所以说拓扑连接是一种广义的数据结构。

3.  提取拓扑图空间特征的两种方式

   Spatial domain和Spectral domain实现目标是两种最主流的方式

   （1） Spatial domain 是一种很直观的方式，提取拓扑图上的空间特征，就把每个顶点相邻的neighbors找出来。这里存在两个问题：

   a.按照什么条件去找中间vertex的neighors， 也就是如何确定receptive field

   b.确定receptive field, 按照什么方式处理包含不同数目neighbors的特征

   这种方法 主要的缺点如下：

   c. 每个顶点提取出来的neighbors不同，使得计算处理必须针对每个顶点

   d. 提取特征的效果可能没有卷积好

   （2）Spectral domain就是GCN的理论基础了，这种思路希望借助图谱的理论来实现拓扑图上的卷积操作。

   Q1 什么是Spectral graph theory?

   借助于图的拉普拉斯矩阵的特征值和特征向量来研究图的性质

   Q2 GCN为什么要利用Spectral graph theory?

   先来看Spectral graph实现了什么，再进行探究为什么？

4. 什么是拉普拉斯矩阵？为什么要GCN要用拉普拉斯矩阵?

   对于图G=(V,E), 其Laplacian矩阵的定义为L=D-A, 其中D是顶点的度矩阵，A是图的邻接矩阵 。

   常用的拉普拉斯矩阵实际有三种

   No.1 Combinatorial Laplacian
   $$
   L=D-A
   $$
   No.2 Symmetric normalized Laplacian, 很多GCN论文中应用的是这种拉普拉斯矩阵
   $$
   L^{sys}=D^{-1/2}LD^{-1/2}
   $$
   No.3 Random walk normalized Laplacian
   $$
   L^{rw}=D^{-1}L
   $$
   为什么GCN要用拉普拉斯矩阵

   （1) 拉普拉斯矩阵是对称矩阵，可以进行特征分解，这就和GCN的spectral domain对应上了

   （2）拉普拉斯矩阵只在中心顶点和一阶相连的顶点上有非0元素，其余之处均为0

   （3) 通过拉普拉斯算子与拉普拉斯矩阵进行类比

   5. 拉普拉斯矩阵的谱分解（特征分解）

   矩阵的谱分解。特征分解。对角化都是同一个概念

   不是所有的矩阵都可以特征分解，其充要条件为n阶方阵存在n个线性无关的特征向量

   但是拉普拉斯矩阵是半正定矩阵（半正定矩阵本身就是对称矩阵），有如下三个性质：

   - 对阵矩阵一定n个线性无关的特征向量
   - 半正定矩阵的特征值一定非负
   - 对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵

   特征分解最右边的是特征矩阵的逆，只是拉普拉斯矩阵的性质才可以写成特征矩阵的转置。

   6. 如何从传统的傅里叶变换、卷积类比到Graph上的傅里叶变换及卷积
   7. 为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？
   8. Deep Learning中的Graph Convolution

   9. 在GCN中的Local Connectivity和Parameter Sharing



###  Reference
> 【1】Graph Neural Network（GNN）综述 https://mp.weixin.qq.com/s/LrGWJIdPdUNZ3jyC8tdE6w
> 【2】Graph Neural Network Review（PPT）版 https://zhuanlan.zhihu.com/p/64018075
> 【3】图神经网络综述：模型与应用 https://mp.weixin.qq.com/s/DIdJb_vG8_zLdp_t56jLXQ
> 【4】图神经网络为何如此强大 https://mp.weixin.qq.com/s/DUv5c6ce-dgLOBAE4ChiQg
> 【5】跳出公式，看清全局，图神经网络（GCN）原理详解 
> https://mp.weixin.qq.com/s/0Ztt1ch57dpBD7nC4L5G7Q
> 【6】如何理解 Graph Convolutional Network（GCN）？https://www.zhihu.com/question/54504471
> 【7】图卷积网络到底怎么做，这是一份极简的Numpy实现 https://mp.weixin.qq.com/s/sg9O761F0KHAmCPOfMW_kQ
> 【8】graph convolutional network有什么比较好的应用task https://www.zhihu.com/question/305395488