---
layout:     post                    # 使用的布局
title:      Recurrent Neural Network               # 标题 
subtitle:   #副标题
date:       2019-01-04              # 时间
author:     Doublefierce                      # 作者
header-img: img/bg-post.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - RNN
---

### 循环神经网络教程

>**第一部分-RNN简介**
>RNN背后的思想是利用顺序信息。在传统的神经网络中，我们假设所有的输入（包括输出）之间是相互独立的。对于很多任务来说，这是一个非常糟糕的假设。如果你想预测一个序列中的下一个词，你最好能知道哪些词在它前面。RNN之所以循环的，是因为它针对系列中的每一个元素都执行相同的操作，每一个操作都依赖于之前的计算结果。换一种方式思考，可以认为RNN记忆了到当前为止已经计算过的信息。理论上，RNN可以利用任意长的序列信息，但实际中只能回顾之前的几步。
>https://zhuanlan.zhihu.com/p/22266022
>**第二部分-用python，numpy，theano实现一个RNN**
>https://zhuanlan.zhihu.com/p/22289383
>**第三部分-BPTT和梯度消失**
>你可以看到tanh和sigmoid函数在两端的梯度值都为0，接近于平行线。当这种情况出现时，我们就认为相应的神经元饱和了。它们的梯度为0使得前面层的梯度也为0。矩阵中存在比较小的值，多个矩阵相乘会使梯度值以指数级速度下降，最终在几步后完全消失。比较远的时刻的梯度值为0，这些时刻的状态对学习过程没有帮助，导致你无法学习到长距离依赖。消失梯度问题不仅出现在RNN中，同样也出现在深度前向神经网中。只是RNN通常比较深（例子中深度和句子长度一致），使得这个问题更加普遍。
>https://zhuanlan.zhihu.com/p/22338087
>**第四部分-用Python和Theano实现GRU/LSTM循环神经网络**
>https://zhuanlan.zhihu.com/p/22371429
>原文链接：http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/

### 理解 LSTM 网络 （Understanding LSTM Networks）

>https://blog.csdn.net/jerr__y/article/details/58598296
>原文链接：https://colah.github.io/posts/2015-08-Understanding-LSTMs/

### 递归神经网络不可思议的有效性

>https://www.csdn.net/article/2015-08-28/2825569
>原文链接：http://karpathy.github.io/2015/05/21/rnn-effectiveness/

### The Ultimate Guide to Recurrent Neural Networks (RNN)

> https://www.superdatascience.com/blogs/the-ultimate-guide-to-recurrent-neural-networks-rnn

### LSTM部分问题汇总详解

>https://blog.csdn.net/ZJRN1027/article/details/80301039

### RNN之多层LSTM理解：输入，输出，时间步，隐藏节点数，层数

>https://blog.csdn.net/weixin_41041772/article/details/88032093